{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import os , csv\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= '/media/maria/DATA1/Documents/NeuroMatchAcademy2020_dat/unzipped_files/Richards_2017-10-31.tar'\n",
    "\n",
    "trials_intervals = np.load(data_path+'/'+'trials.intervals.npy')*1000 # in seconds\n",
    "spike_times = np.load(data_path+'/'+'spikes.times.npy') * 1000 # Unbinned spike times in ms\n",
    "trials_gocue_times = np.load(data_path+'/'+'trials.goCue_times.npy') \n",
    "trials_response_choice = np.load(data_path+'/'+'trials.response_choice.npy') # -1 left, 1, right, 0 no response\n",
    "spontaneous_intervals = np.load(data_path+'/'+'spontaneous.intervals.npy')\n",
    "trials_response_time = np.load(data_path+'/'+'trials.response_times.npy')\n",
    "spike_clusters = np.load(data_path+'/'+'spikes.clusters.npy')\n",
    "site_positions = np.load(data_path+'/'+'channels.sitePositions.npy')\n",
    "clusters_depths = np.load(data_path+'/'+'clusters.depths.npy')\n",
    "clusters_annotation = np.load(data_path+'/'+'clusters._phy_annotation.npy')\n",
    "channel_sites = np.load(data_path+'/'+'channels.site.npy')\n",
    "channels_brainlocation = pd.read_csv(data_path+'/'+'channels.brainLocation.tsv', sep='\\t')\n",
    "clusters_probes = np.load(data_path+'/'+'clusters.probes.npy')\n",
    "channels_probe = np.load(data_path+'/'+'channels.probe.npy')\n",
    "trials_visual_time = np.load(data_path+'/'+'trials.visualStim_times.npy')\n",
    "\n",
    "visual_times = trials_visual_time\n",
    "# Behaviour data\n",
    "\n",
    "wheel_movement = np.load(data_path+'/'+'wheelMoves.type.npy')\n",
    "wheel_intervals = np.load(data_path+'/'+'wheelMoves.intervals.npy')\n",
    "\n",
    "# Taken from https://github.com/MouseLand/steinmetz2019_NMA/blob/master/steinmetz_loader.py\n",
    "# To obtain brain regions\n",
    "def get_good_cells(fdirpath): #\n",
    "    # location in brain of each neuron\n",
    "    brain_loc = os.path.join(fdirpath, \"channels.brainLocation.tsv\")\n",
    "\n",
    "    good_cells = (np.load(os.path.join(fdirpath, \"clusters._phy_annotation.npy\")) >= 2 ).flatten()\n",
    "    clust_channel = np.load(os.path.join(fdirpath, \"clusters.peakChannel.npy\")).astype(int) - 1\n",
    "    br = []\n",
    "    with open(brain_loc, 'r') as tsv:\n",
    "        tsvin = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        k=0\n",
    "        for row in tsvin:\n",
    "            if k>0:\n",
    "                br.append(row[-1])\n",
    "            k+=1\n",
    "    br = np.array(br)\n",
    "    good_cells = np.logical_and(good_cells, clust_channel.flatten()<len(br))\n",
    "    brain_region = br[clust_channel[:,0]]\n",
    "\n",
    "\n",
    "    return good_cells, brain_region, br\n",
    "\n",
    "# Bin cells according to input bin size\n",
    "\n",
    "def latency_coding(spike_times,trials_intervals, bin_size = 100):\n",
    "    \n",
    "    # Using clusters._phy_annotation.npy obtain valid clusters (i.e. >= 2)\n",
    "    valid_clusters_idx = np.array(np.where(clusters_annotation>=2))[0]\n",
    "\n",
    "    spike_time_cells = np.empty(len(valid_clusters_idx), dtype=object) # Initalise empty object\n",
    "    for i,cell_idx in enumerate(valid_clusters_idx):\n",
    "      # Create a spike time arrays, where each array in the array is a spike time of a cell\n",
    "      spike_time_cells[i] = spike_times[(np.where(spike_clusters == cell_idx)[0])]\n",
    "\n",
    "    #print(spike_time_cells)\n",
    "    #print(spike_time_cells[0].shape)\n",
    "    #print(spike_time_cells[1].shape)\n",
    "    \n",
    "    first_spike_arr=[]\n",
    "    for neuron in range(spike_time_cells.shape[0]):\n",
    "        spk_tms_one_neuron=spike_time_cells[neuron]\n",
    "        #print(spk_tms_one_neuron)\n",
    "        first_spike_arr_trial=[]\n",
    "        for trial_index in range(trials_intervals.shape[0]):\n",
    "            #print(trials_intervals)\n",
    "            spks_range = np.bitwise_and(spk_tms_one_neuron>=trials_intervals[trial_index][0],spk_tms_one_neuron<=trials_intervals[trial_index][1])\n",
    "            #print(spks_range)\n",
    "            spk_lst=list(spk_tms_one_neuron[spks_range])\n",
    "            if not spk_lst:\n",
    "                spk=100000\n",
    "            else:\n",
    "                spk=np.sort(spk_lst)[0]\n",
    "                spk=spk-trials_intervals[trial_index][0]\n",
    "            first_spike_arr_trial.append(spk)\n",
    "        first_spike_arr.append(first_spike_arr_trial)\n",
    "            \n",
    "    return np.array(first_spike_arr).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_spike_arr=latency_coding(spike_times,trials_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00790519e+03  5.98605188e+02  1.88505188e+02 ...  8.08088380e+01\n",
      "   4.91086923e+01  3.98419830e+01]\n",
      " [ 2.54008887e+02  1.46208887e+02  2.47875554e+02 ...  9.08761843e+01\n",
      "   1.18242977e+02  2.54492079e+03]\n",
      " [ 1.78063012e+03  5.12263455e+02  2.58443012e+03 ...  1.52800370e+01\n",
      "   4.47515357e+02 -1.00000000e+00]\n",
      " ...\n",
      " [ 1.03247215e+01 -1.00000000e+00  2.35247215e+01 ...  4.23424853e+01\n",
      "   9.17566624e+00  1.18442835e+02]\n",
      " [ 1.05034242e+02  2.87663424e+03  4.47267576e+02 ...  1.28469367e+02\n",
      "   5.98023844e+01  1.24257449e+03]\n",
      " [ 9.40599956e+01  2.46326662e+02  1.42726662e+02 ...  1.64766393e+01\n",
      "   5.84434989e+01  2.08977524e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(first_spike_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regr_spike_lat(first_spike_arr,trials_response_choice):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(first_spike_arr, trials_response_choice, test_size=0.33, random_state=42)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "    print(clf.score(X_train,y_train))\n",
    "    print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.686046511627907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logistic_regr_spike_lat(first_spike_arr,trials_response_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trials_intervals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trials_response_choice.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
