{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import os , csv\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load desired data from 1 session 1 animal\n",
    "# Note that there are 340 trials in 1 session\n",
    "# (For more info https://github.com/nsteinme/steinmetz-et-al-2019/wiki/data-files)\n",
    "\n",
    "data_path= '/Users/xinweichia/Documents/connected_lizards/Steinmetz_dataset/Richards_2017-10-31'\n",
    "\n",
    "os.chdir(data_path) # Change working directory\n",
    "\n",
    "\n",
    "trials_intervals = np.load('trials.intervals.npy') # in seconds\n",
    "spike_times = np.load('spikes.times.npy') * 1000 # Unbinned spike times in ms\n",
    "trials_gocue_times = np.load('trials.goCue_times.npy') \n",
    "trials_response_choice = np.load('trials.response_choice.npy') # -1 left, 1, right, 0 no response\n",
    "spontaneous_intervals = np.load('spontaneous.intervals.npy')\n",
    "trials_response_time = np.load('trials.response_times.npy')\n",
    "spike_clusters = np.load('spikes.clusters.npy')\n",
    "site_positions = np.load('channels.sitePositions.npy')\n",
    "clusters_depths = np.load('clusters.depths.npy')\n",
    "clusters_annotation = np.load('clusters._phy_annotation.npy')\n",
    "channel_sites = np.load('channels.site.npy')\n",
    "channels_brainlocation = pd.read_csv('channels.brainLocation.tsv', sep='\\t')\n",
    "clusters_probes = np.load('clusters.probes.npy')\n",
    "channels_probe = np.load('channels.probe.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/MouseLand/steinmetz2019_NMA/blob/master/steinmetz_loader.py\n",
    "# To obtain brain regions\n",
    "def get_good_cells(fdirpath): #\n",
    "    # location in brain of each neuron\n",
    "    brain_loc = os.path.join(fdirpath, \"channels.brainLocation.tsv\")\n",
    "\n",
    "    good_cells = (np.load(os.path.join(fdirpath, \"clusters._phy_annotation.npy\")) >= 2 ).flatten()\n",
    "    clust_channel = np.load(os.path.join(fdirpath, \"clusters.peakChannel.npy\")).astype(int) - 1\n",
    "    br = []\n",
    "    with open(brain_loc, 'r') as tsv:\n",
    "        tsvin = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        k=0\n",
    "        for row in tsvin:\n",
    "            if k>0:\n",
    "                br.append(row[-1])\n",
    "            k+=1\n",
    "    br = np.array(br)\n",
    "    good_cells = np.logical_and(good_cells, clust_channel.flatten()<len(br))\n",
    "    brain_region = br[clust_channel[:,0]]\n",
    "\n",
    "\n",
    "    return good_cells, brain_region, br\n",
    "\n",
    "good_cells, brain_regions ,br = get_good_cells('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising spike time dataset\n",
    "\n",
    "# Using clusters._phy_annotation.npy obtain valid clusters (i.e. >= 2)\n",
    "valid_clusters_idx = np.array(np.where(clusters_annotation>=2))[0]\n",
    "\n",
    "spike_time_cells = np.empty(len(valid_clusters_idx), dtype=object) # Initalise empty object\n",
    "for i,cell_idx in enumerate(valid_clusters_idx):\n",
    "  # Create a spike time arrays, where each array in the array is a spike time of a cell\n",
    "  spike_time_cells[i] = spike_times[(np.where(spike_clusters == cell_idx)[0])]\n",
    "\n",
    "# Bin spike times into 10ms intervals\n",
    "bin_size = 10\n",
    "\n",
    "spike_time_binned = np.empty(len(valid_clusters_idx), dtype=object) # Initalise empty object\n",
    "sum_spikes = np.empty(len(valid_clusters_idx), dtype=object) # Initalise empty object\n",
    "\n",
    "for cell_num in np.arange(len(spike_time_cells)): \n",
    "  spike_time_hist = np.histogram(spike_time_cells[cell_num],bins = np.arange(0,np.floor(spike_time_cells[cell_num][-1]),bin_size))\n",
    "  spike_time_binned[cell_num] = spike_time_hist[0]\n",
    "  sum_spikes[cell_num] = np.sum(spike_time_binned[cell_num])\n",
    "\n",
    "\n",
    "cell_spikes_max = np.argmax(sum_spikes) # cell with the maximum number of spikes for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort cells into trial types and relevant epoch\n",
    "\n",
    "# Get response choice trials types\n",
    "right_choice_trials = np.where(trials_response_choice == -1)[0]\n",
    "left_choice_trials = np.where(trials_response_choice == 1)[0]\n",
    "no_response_choice_trials = np.where(trials_response_choice == 0)[0]\n",
    "\n",
    "response_epoch_duration  = 400 * 0.1 # 400ms in 10ms bins\n",
    "\n",
    "# Sort into trials\n",
    "spike_time_binned_trial = np.empty(len(spike_time_cells), dtype=object)\n",
    "spike_time_binned_trial_response = np.empty(len(spike_time_cells), dtype=object)\n",
    "for cell_num in np.arange(len(spike_time_cells)):\n",
    "  spike_time_binned_trial[cell_num] = np.empty(len(trials_intervals), dtype=object)\n",
    "  spike_time_binned_trial_response[cell_num] = np.empty(len(trials_intervals), dtype=object)\n",
    "  for i,trials_start_end in enumerate(trials_intervals):\n",
    "    # Sort spikes into their trial numbers. \n",
    "    spike_time_binned_trial[cell_num][i] = spike_time_binned[cell_num][ int(np.floor(trials_start_end[0]*100)) : int(np.floor(trials_start_end[1]*100))]\n",
    "    # Using Go_cue time to splice a trial into gocue onset : gocue onset +400ms\n",
    "    spike_time_binned_trial_response[cell_num][i] = spike_time_binned[cell_num][(int(np.floor(trials_gocue_times[i]*100))) : (int(np.floor(trials_gocue_times[i]*100) + epoch_duration))]\n",
    "\n",
    "\n",
    "# Sort trials into response type\n",
    "\n",
    "left_response_spike_time_response_epoch = The \n",
    "for cell_num in np.arange(len(spike_time_cells)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
